# Раздел. Обратный метод настройки весов и порогов

Можно использовать, если входное пространство образов является двумерным.

Рассмотрим задачу логическое ИЛИ. Построим в этот раз от разделяющей линии, а не от обучения заполнением рандомом.

![картинка]()

$x_2 = k x_1 + b$

$x_1 = 0 \Rightarrow x_2 = 0.5 \Rightarrow 0.5 = k * 0 + b \Rightarrow b = 0.5$

$x_1 = 0.5 \Rightarrow x_2 = 0 \Rightarrow 0 = k * 0.5 + 0.5 \Rightarrow k = {- 0.5 \over 0.5} = -1$

$x_2 = - x_1 + 0.5$

От уравнения прямой линии к взвешенной сумме:

1-ый вариант) $S_1 = x_2 + x_1 - 0.5$. Если $S_1 = 0$, то получим $x_2 = - x_1 + 0.5$

2-ой вариант) $S_2 = - x_2 - x_1 + 0.5$. Если $S_2 = 0$, то получим $x_2 = - x_1 + 0.5$

Из двух взвешенных сумм получим

$x_1 = 0 \Rightarrow x_2 = 0 \Rightarrow S_1 = 0 + 0 - 0.5 = - 0.5 < 0$

Из двух вариантов выбираем $S_1 = x_1 + x_2 - 0.5$

$S_1 = x_1 \omega_1 + x_2 \omega_2 - T \Rightarrow \omega_1 = 1, \omega_2 = 1, T = 0.5$

![Картинка]()

| $x_1$ | $x_2$ | $ S $ | $ y $ |
|:-----:|:-----:|:-----:|:-----:|
| 0     | 0     | -0.5  | 0     |
| 0     | 1     | 0.5   | 1     |
| 1     | 0     | 0.5   | 1     |
| 1     | 1     | 1.5   | 1     |

<div style="page-break-after: always;"></div>

# Раздел. Правило обучения Видроу-Хоффа (Дельта правило)

Используется для обучения линейного однослойного персептрона.

**<u>Линейным пересептроном</u>** называется нейронные элементы обрабатывающего слоя, которого имеют линейную функцию активации.

![Картинка]()

$$
y_i = S_j = \sum_{i = 1}^n \omega_{ij} x_i - T_j
$$

Целью обучения сетя является минимизация суммарной квадратичной ошибки сети, которая характеризует разницу между реальными выходными значениями и эталоными для всех образов из обучающей выборки.

$$
E_s = {1 \over 2} \sum_{k = 1}^L \sum_{j = 1}^m (y_j^k - e_j^k)^2 \to min
$$

$ L $ - Кол-во образов обучающей выборки

Обучающая выборка

$$
X = \left[
    \begin{array}{c}
      x^1\\
      x^2\\
      ...\\
      x^L
    \end{array}
  \right]

  =

  \left[
  \begin{array}{c c c c}
  x_1^1 & x_2^1 & ... & x_n^1 \\
  x_1^2 & x_2^2 & ... & x_n^2 \\
  - & - & - & - \\
  x_1^L & x_2^L & ... & x_n^L \\
 \end{array}
 \right]

 \begin{array}{l}
  -~1~obraz \\
  -~2~obraz \\
  - \\
  -~L~obraz \\
 \end{array}

$$

$$
e = \left[
    \begin{array}{c}
      e^1\\
      e^2\\
      ...\\
      e^L
    \end{array}
  \right]

  =

  \left[
  \begin{array}{c c c c}
  e_1^1 & e_2^1 & ... & e_m^1 \\
  e_1^2 & e_2^2 & ... & e_m^2 \\
  - & - & - & - \\
  e_1^L & e_2^L & ... & e_n^L \\
 \end{array}
 \right]
$$

Обеспечение $x^k$ в эталонные $x^k \to e^k$, $\forall k = \bar{1,L}$

$(\omega, T) = \min E$ - найти такие $\omega, T$, когда $\min E$

Для минимизации сумарной квадратичной ошибки сети будем использовать метод градиентного спуска в пространстве весов и порогов.

$y = x^2$

![картинка]()

1 метод) взятие производной и приравнивание к нулю

2 метод) $x(t+1) = x(t) - \alpha {\partial y \over \partial x}$

Для $y = x^2$ по второму методу $x(t+1) = x(t) - \alpha 2 x t$

![картинка]()

$\alpha$ - шаг обучения / скорость обучения

Существует два подхода к обучению:

1. Online learning - последовательный метод обучения
2. Batch learning - групповой метод обучения

<div style="page-break-after: always;"></div>

# Раздел. Последовательное обучение

В этом случае модификация весов и порогов нейронной сети происходит после подачи каждого образа из обучающей выборки на нейронную сеть. Тогда в методе градиентного спуска используется квадратичная ошибка для одного образа

$$
E = {1 \over 2} \sum_{j = 1}^m (y_j - e_j)^2
$$

Для минимизации суммарной квадратичной ошибки сети в соотвествии с методом градиентного спуска веса и пороги должны изменяться следующим образом:

$$
\omega_{ij}(t+1) = \omega_{ij}(t) - \alpha {\partial E \over \partial \omega_{ij}}
$$

$$
T_j(t+1) = T_j(t) - \alpha {\partial E \over \partial T_j}
$$

$$
y_j = \sum_{i = 1}^n \omega_{ij} x_i - T_j
$$

$$
{\partial E \over \partial \omega_{ij}} = {\partial E \over \partial y_j} {\partial y_j \over \partial \omega_{ij}}
$$

$$
{\partial E \over \partial y_j} = y_j - e_j
$$

$$
E = {1 \over 2} (y_1 - e_1)^2 + {1 \over 2} (y_2 - e_2)^2
$$

$$
{\partial E \over \partial y_2} = y_2 - e_2
$$

$$
{\partial y_j \over \partial \omega_{ij}} = x_i
$$

$$
{\partial E \over \partial \omega_{ij}} = x_i (y_j - e_j)
$$

По порогам:

$$
{\partial E \over \partial T_j} = {\partial E \over \partial y_j} {\partial y_j \over \partial T_j} = - (y_j - e_j)
$$

Подставляем в исходное выражение для градиентного спуска - и получаем следующие:

$$
\omega_{ij}(t+1) = \omega_i(t) - \alpha x_i (y_j - e_j)
$$

$$
T_j(t+1) = T_j(t) + \alpha (y_j - e_j)
$$

Это правило Видроу Хоффа или дельта правило.

<div style="page-break-after: always;"></div>

# Раздел. Алгоритм обучения

1. Случайная инициализация весов и порогов в достаточно узком диапазоне значений $\omega, T \in [0;1]$
2. Задание шага обучения $\alpha = const \in (0;1)$ и значения желаемой суммарной квадратичной ошибки сети $E_e$ до уровня, которой мы хотим обучить сеть.
3. Последовательно подаются входные образы из обучающей выборки на нейронную сеть. $k = \bar{1, j}$ - кол-во образов обучающей выборки. И для каждого образа производятся следующие действия:
   1. Вычисляются выходные значения сети:
      
      $$
      y_j = \sum_{i = 1}^n \omega_{ij} x_i - T_{ij}
      $$
      
      $j = \bar{1,m}$
   2. В соотвествии с дельта правилом производится модификация весов и порогов
      
      $$
      \omega_{ij}(t+1) = \omega_{ij}(t) - \alpha x_i (y_j - e_j)
      $$
      
      $$
      T_j(t+1) = T_j(t) + \alpha (y_j - e_j)
      $$
      
      $i = \bar{1,n}$, $j = \bar{1,m}$
4. Последовательно подаються входные образы из обучающей выборки на нейронную сеть $k=\bar{1,n}$ и вычисляются значения сумарной квадратичной ошибки сети
   
   $$
   E_s = {1 \over 2} \sum_{k = 1}^L \sum_{j=1}^m (y_j^k - e_j^k)^2
   $$
5. Происходит сравнение суммарной квадратичной ошибки сути с желаемой. Если $E_S > E_e$, то алгоритм продолжается начиная с пункта 3, в противном случае обучение заканчивается

**Примечание**. Другой критерий остановки алгоритма

Алгоритм обучения щаканчивается, когда перестаёт уменьшатся значение суммарной квадратичной ошибки сети или её уменьшение происходит не значительно

![картинка]()

<div style="page-break-after: always;"></div>

# Раздел. Груповое обучение

В этом случае модификация весов и порогов происходит после подачи группы образов на нейронную сеть.

Пусть размер группы образов, после которой происходит модификация весов и порогов $< 1$.

Если $L > L_1 \geq L$, то такой метод обучения называется методом стохастического градиента SGD (Stochastic gradient descent). В этом случае в методе градиентного спуска используется квадратичная ошибка для $L_1$ образов.

$$
E(L_1) = {1 \over 2} \sum_{k=1}^{L_1} \sum_{j = 1}^m (y_j^k - e_j^k)^2
$$

Тогда для минимизации суммарной квадратичной ошибки сети веса и пороги должны изменятся следующим образом:

$$
\omega_{ij} (L_1) = \omega_{ij}(0) - \alpha {\partial E(L_1) \over \partial \omega_{ij}}
$$

$$
T_j(L_1) = T_j(0) - \alpha {\partial E(L_1) \over \partial T_j}
$$

Физический смысл записи

$\omega_{ij}(L_1)$ - веса изменяються после L образа

$\omega_{ij}(0)$ - веса до подачи в нейронную сеть

$$
y_j^k = \sum_{i = 1}^n \omega_{ij} x_i^k - T_j
$$

$$
{\partial E(L_1) \over \partial \omega_{ij}} = \sum_{k=1}^{L_1} {\partial E(L_1) \over \partial y_j^k} {\partial Y_j^k \over \partial \omega_{ij}}
$$

$$
{\partial E(L_1) \over \partial \omega_{ij}} = \sum_{k=1}^{L_1} (y_j^k - e_j^k) x_i^k
$$

$$
{\partial E(L_1) \over \partial T_j} = \sum_{k=1}^{L_1} (y_j^k - e_j^k)
$$

В результате можем получить дельта правило для группового обучения:

$$
\omega_{ij}(L_1) = \omega_{ij}(0) - \alpha \sum_{k=1}^{L_1} x_i^k (y_j^k - e_j^k)
$$

$$
T_j(L_1) = T_j(0) + \alpha \sum_{k=1}^{L_1} (y_j^k - e_j^k)
$$