# Классификация нейронный сетей

1. Персептронные нейронный сети

	Характеризуются обучением учителя
	
	[картинка]

	- однослойный персептрон

		SLP - single layer perceptron
		
		1960 - Френрих
		
	- многослойный персептрон

		MLP - multi layer perceptron

			1986 - предложен многослойный персептрон
			
	- сверточные нейронные сети

		CNN - convolutional neural network
		
	- рекулентные нейронные сети

		RNN - recurned neural network
		
	- автоэнкодерные нейронные сети

2. Самоорганизующие нейронные сети

	Характеризуются обучением без учителя
	
	[картинка]
	
	- нейроные сети Кохонен-а
	- нейронные сети адаптивного резонанса
		ART - adaptive rezonanse theory
		
3. Нейроные сети обучающие сигналом подкрепления от внешней среды
	RL - rein forcement learning
	
	[картинка]
	
4. Нейронные сети ассоциативной памяти
	- нейроная сеть Холфилд-а
	- нейроная сеть Хэмминг-а
	- двунаправленная ассоциативная память
	
5. Гибридные нейронные сети

	Характеризуются использованием различных подходов к обучению
	
	- RBF - нейронные сети с радиальной машиной функцией активации
	- нечёткие нейронные сети
	
		Характеризуются приминением нечёткой логики нейронных сетей
		
		Fuzzy logic
		
	- искусственно имуные системы
	- генетические алгоритмы
	- эволюционное программирование
	
# [Раздел] Искуственный нейроная

Искусственный нейрон осуществляет операцию не линейного преобразования суммой произведений входных сигналов на весовые коэффициенты

[картинка]

$W = [W1 W2 .. W_n]$

Сумма произведений входных сигналов на весовые коэффициенты - звешаная сумма

$S = \sum^n_{i = 1} \omega_i x_i$

$X = [X1 X2 .. X_n]$ - вектор входного сигнала

F - оператор не линейного преобразования, который называется функцией активации нейроного элемента

$Y = F(s)$

Обучение нейронного элемента происходит при помощи модификации весовых весовых коэффициентов.

$\omega_1, \omega_2, \omega_3$ - настраиваемые параметры сети

Взвешанная сумма нейрона представить в виде скалярного произведения весового вектора на входной вектор.

$S = \sum^n \omega_i x_i = (\omega, x) = |\omega| |x| cos \omega, \^ x$

$\omega$ - длина весового вектора

$|\omega| = \sqrt{ \omega_1^2 + \omega_2^2 + .. \omega_n^2 }$

$ x $ - входной вектор

$|x| = \sqrt{ x_1^2 + x_2^2 + .. x_n^2 }$

# [Раздел] Функций активаций нейроных элементов

Пусть $ T $ - пороговое значение нейроного элемента, тогда взвешеную сумму можно представить

$S = S' - T = \sum^n_{i = 1} \omega_1 x_i - T$

$\omega, T$ - настраиваемые параметры сети. Настраиваются в процессе обучения

Пороговое значение характеризует сдвиг функции активации по оси абсцисс в ту или иную сторону.

Существует функции активации

1. Пороговая функция активации

- бинарная пороговая функция активации

[картинка]

Эти функции не дифференцируемы и не применимы математические операции поэтому предложены:

2. Непрерывные функции активации

- сигноидная функция активации

$y = F(S) = {1 \over 1 + e^{-S}}$

$s = 0 y = 0.5$

$s \to \infinity y \to 1$

$S \to \infiniey y \to 0$

[img]

- биполярная сигноидная функция активации

$y = F(S) = {2 \over 1 + e^{-S}} - 1$

$s = 0 y = 0$

$s \to \infinity y \to 1$

$S \to \infiniey y \to -1$

- гиперболический тангенс

$y = F(S) = { e^S - e^{-S} \over e^S + e^{-S} } = {2 \over 1 + e^{-2S}} - 1$

- линейная функция

$y = S$

3. Рефлекационая функция автивации 

ReLU - rectifield linear unit

[img]

# [Раздел] Однослойный персептрон

Однослойный персептрон принято изображать в виде двух слоев нейроных элементов

1-ый слой является входным (распределительным). Основная задача его - распределять входные сигналы на нейронные элементы следующего слоя.

2-ой слой яввляется обрабатывающим. Каждый нейронраспределительного слоя имеет связи со всеми нейронами обрабатывающего слоя.

[img]

$y_i = F(S_i)$

Взвешенная сумма $S_j = \sum^n_{i = 1} \omega_{ij} x_i - T_j$

[картинка]

$S_2 = X_1 \omega_{12} + x_2 \omega_{22} + .. x_1 \omega{n2} - T_2$

Представление веса или весовые коэффициенты в виде матрицы

[img]

# [Раздел] Возможности однослойного персептрона

[img]

$S = x_1 \omega_1 + x_2 \omega_2 - T$

Примичание. Пороговые значения можно вынести за знак нейроного элемента и рассматривать его как вес

[img]

Будем использовать бинарную функцию активации